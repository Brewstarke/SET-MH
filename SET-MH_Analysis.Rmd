---
title: Surface Elevation Table- Marker Horizon (SET-MH) 

author: "Adam Starke"
subtitle: The Nature Conservancy - Long Island Chapter
output: html_notebook
---

```{r DocumentSetup, message=TRUE, include=FALSE, results='asis'}
# Data loading/munging
# library(readr)
library(RODBC)
# library(dplyr)
# library(tidyr)
library(tidyverse)
library(broom)
library(magrittr)
library(stringr)
library(xlsx)
library(xts)
# library(purrr)
library(tidyquant)
library(ggExtra)
library(lubridate)

# Graphics
# library(ggplot2)
library(png)
library(RColorBrewer)
library(colorRamps)
library(ggthemes)
library(dygraphs)
library(xtable)
library(rmarkdown)
library(tufte)
library(tint)
library(taucharts)


# Spatial packages
library(leaflet)
library(rgdal)
library(rgeos)
library(sp)
library(spatial)
library(maptools)

library(htmltools)



source("02_helpers.R")
```

## Current Data and Analysis File Structure
This document is intended to document the process used for database storage and data analysis of our SET-MH data. Our current workflow uses Access2010 to store the data and RStudio (running the R language) for data handling. R is a powerful, though often times overwhelming tool. It is hoped that this document will aid anyone unfamiliar with R through our current data processing methods.


### Data Storage

The SET-MH method of marsh monitoring generates a large set of data that needs to be carefully maintained and catalogued. Fortunately, there has been a substaintial effort put forth by the National Park Service's National Capital Region Network Inventory and Monitoring Program to create a user-friendly Access database (DB). This DB allows for easy field data entry, data browsing and the creation of summary plots. There are also methods for exporting data to other software if desired. Managing SET-MH data in such as system is critical.  
Although the statistical analyses needed to calculate trends in marsh elevation are somewhat basic the restructuring of the data to allow for analysis can be somewhat complex. With this as a characteristic of the data we decided to use the statistical programming language R. R is one of the most widely used programming languages and contains an enormous amount of tools and scripts (known as packages in R) that make these restructuring of data possible. The ability to create 'scripts' allow for quick updates to analysis after data entry. A number of other features also exist that make R a good choice, particulary the graphics capabilities and the support for creation of 'data-driven documents' which allows for reproducible research. This document is actually written with R code embedded within it. This allows one to actually run the code that performs the analysis along side the documentation of the code. This allows the user to describe and show the exact steps taken for data organization, restructuring and analysis. Actual R code run in this document will be highlighted and have a slight grey background. This code can be cut and pasted or run as a whole document.  

```{r DataLoad, message=FALSE, warning=FALSE, include=FALSE}
## @knitr DataLoad

###
# Data importing from SET_Monitoring_Database 'Backend Data Storage File' currently SET_DB_BE_ver_2.92_TNC_Master
# ProjectTemplate automatically runs this script, importing the dataframes below into the R workspace
#
###

# C:\Users\astarke\Desktop\Box Sync\SET_Project\SET_DatabaseStorage\Database_storage
SET.DB.path <- "C:\\Users\\astarke\\Desktop\\Box Sync\\SET_Project\\SET_DatabaseStorage\\Database_storage\\SET_DB_BE_ver_2.94_TNC_Master.mdb"
# SET.DB.path <- choose.files(caption = "Select SET-MH Database storage file (back-end file for Access)", multi = FALSE)

SET.DB <- odbcConnectAccess2007(SET.DB.path)

## @knitr DataTableLoad

### Pull in the tables of interest containing data needed for analysis ====
#Location data- i.e. study sites and stations within sites
Sites <- sqlFetch(SET.DB, sqtable= "tbl_Sites")
Locations <- sqlFetch(SET.DB, sqtable= "tbl_Locations")

# Sites and Locations will be joined in munge by 'site_ID'
# Use capwords function to standardize caps and revert Stratafication to as a factor
Locations$Stratafication <- as.factor(capwords((as.character(Locations$Stratafication))))

# Sampling events
Events <- sqlFetch(SET.DB, sqtable= "tbl_Events")

# Contacts/Field Personel
EventContacts <- sqlFetch(SET.DB, sqtable = "xref_Event_Contacts")
Contacts <- sqlFetch(SET.DB, sqtable = "tlu_Contacts")

# SET Rod data
Positions <- sqlFetch(SET.DB, sqtable= "tbl_SET_Position")
SETdata<- sqlFetch(SET.DB, sqtable= "tbl_SET_Data")

# Surface Accretion data
SAccret <- sqlFetch(SET.DB, sqtable= "tbl_Accretion_Data") 
SA_Layers <- sqlFetch(SET.DB, sqtable= "tbl_Feldspar_Layers")

close(SET.DB) #close the connection to the Access SET database.

## @knitr quiet
### END OF SET_data_imports.R script ###
#
# The datatables created here are then 'munged' in the scripts found below.
#
###


```

As mentioned the data are currently stored in an Access database located at:`r  print(SET.DB.path)`

This file is referred to as the 'backend database' which holds all the raw SET-MH data.The 'front-end' of the database contains the user interface (UI) and is linked to the backend allowing multiple 'front ends' all pointing to one dataset. It's suggested that the front-end UI be used for quick checks on data and it is where **ALL** data entry takes place. 

### Accessing Our Data Through R
Using the power of R's scripting language capabilities the quickest, most direct, way to update SET-MH analyses is to simply navigate to the folder containing the RStudio project file (SET-MH_Analysis.Rproj) currently located at: `r getwd()`. This document is stored as a R Notebook and functions as both a documentation of the methods used as well as part of the analysis workflow. 


The main feature in this script is the *ProjectTemplate* package command: load.project() which automatically runs (sources) scripts contained in the '/data' folder, then cleans it up using scripts in the '/munge' folder. Munging simply means to remove irrelevant or missing values, restructure the data, transforming variable names etc. The scripts in each of these folders will be explained below. There are then three other lines of code that source code that performs the actual analysis and creates summary tables and plots. This will also be explained in detail below.  

## Data Processing Steps

This section will walk through each of our data processing steps; from loading data into R to exporting plots and summary tables. Most of these code blocks have been split to allow for commentary on what the code is doing. Again, any text that is bounded and highlighted is actual R code that is run in the software. If needed this text can be copied and pasted into an active R console and run.  

#### Data Loading  

The first step is to connect to the data. To do so R needs to communicate with the Access database through an *"ODBC- protocol"* or translator. For this we use the package **RODBC** which allows R to talk with Access. 


This code block creates a pathway to the Access database that is then used to communicate with the database. Once the connection is established several tables will be pulled into R as 'dataframes'; R's equivelent to a spreadsheet.   

This code block will load 7 tables into R containing:  
* Sites- data associated with each study site, i.e. general location, name, date of establishment  
* Locations- data associated with each station within each study site   
* Events- dates that samples were made  
* Positions- ID's for each SET arm position  
* SETdata- SET readings raw data  
* SAccret- Marker Horizon (MH) surface accretion raw data  
* SA_Layers- MH layer ID information  

#### Munging
Data munging (aka data wrangling) is the process of formatting and rearranging the 'shape' of the data to allow for analysis. As mentioned earlier the actual analysis of SET-MH data is not that complex. The primary crux of the analysis is getting the data into a form that makes the analysis programatically easy. The basic steps are to merge the tables brought in from the DB, retaining all relevant data from each table and then reshaping the data to allow for the analysis to run quickly. 

```{r}
###
#
# Munge the data tables pulled in from Access using the SET_data_imports script found in the ~/data folder. 
# 
### 
## @knitr MungeTables
#SQL type joins to flatten the tables from the database. ----

#Study Sites with Locations data --Location_ID is a numeric key analagous to Plot_Name 
StudyStations <- left_join(Sites, Locations, by="Site_ID")  
StudyStationLocations <- StudyStations %>% 
	select(Location_ID, Site_Name, Stratafication, Plot_Name, X_Coord, 
	       Y_Coord, Coord_System, UTM_Zone, Datum, SET_Type)

#Surface Accretion data 
SA <- inner_join(SA_Layers, SAccret, by="Layer_ID")


#SET Rod data
SET <- inner_join(SETdata, Positions, by="Position_ID")

# SET readers-  Gather up Event_ID, SET Reader and Contact_ID -- Then match up Contact ID with Contacts Last name-
Contacts %<>% select(Contact_ID, Last_Name, First_Name, Organization) %>% mutate(FullName = paste(First_Name, Last_Name, sep = " "))

SET_readers <- EventContacts %>% 
	left_join(Contacts, by = "Contact_ID") %>% 
	select(Event_ID, Contact_ID, Contact_Role, Last_Name, First_Name, Organization, FullName)%>% 
	mutate(ID = paste(Event_ID, Contact_ID, sep = "_")) %>% 
	spread(key = Contact_Role, value = FullName) %>% 
	dplyr::rename(SET_Reader = `SET Reader`) %>% 
	select(Event_ID, Last_Name, First_Name, Organization, SET_Reader) %>% 
	filter(complete.cases(.))


# Sampling events and sites
# join site-location data with sampling 'events'
Samplings <- inner_join(StudyStations, Events, by= "Location_ID") 
Samplings <- inner_join(Samplings, SET_readers, by = "Event_ID")


# Complete datasets with ALL variables
SA.data <- inner_join(SA, Samplings, by="Event_ID")
SET.data <- inner_join(SET, Samplings, by="Event_ID")

#---------------------------------------------------------------------------------------------------------#

# Remove excess dataframes leaving only SA and SET data ----
rm(Events,
   Contacts,
   EventContacts,
   SET_readers,
   Positions,
   SA,
   SA_Layers,
   SAccret,
   SET,
   SETdata,   
   Samplings,
   Sites,
   Locations
   )
## @knitr MungeSETVariables

# Complete SET data in a WIDE format ----
# Trim excess columns and clean charcter strings.
SET.data %<>% tbl_df() %>% 
	select(Pin1:Pin9_Notes, Arm_Direction, Site_Name, SET_Type, Stratafication:Plot_Name, Location_ID.x, Position_ID, Start_Date, Organization, SET_Reader) %>% 
	mutate(Stratafication = capwords(as.character(Stratafication)), Start_Date = as.Date(Start_Date))     # Eventually add a filter that will filter out only 'clean' readings
	
attr(SET.data, 'Datainfo') <-"Full SET dataset including all measures in a WIDE format" # give dataframe some metadata attributes

#---------------------------------------------------------------------------------------------------------#

# Complete SET dataset in a LONG format ----
SET.data.long <- SET.data %>%
	select(Site_Name, Stratafication, Plot_Name, SET_Type, Pin1:Pin9_Notes, Arm_Direction, Location_ID.x, Position_ID, Start_Date, SET_Reader)%>% 
	group_by(Position_ID, Start_Date) %>%
	gather(pin, measure, Pin1:Pin9_Notes) %>% 
	filter(!is.na(measure)) %>% # Remove NA from PinX_Notes 
	separate(pin, c('name', 'note'), "_", remove = TRUE) %>% 
	separate(name, c('name', 'Pin_number'), 3, remove = TRUE) %>% 
	mutate(key = ifelse(is.na(note),yes = "Raw", no = note)) %>% 
	select(-note, -name) %>% 
	spread(key, measure) %>%
	mutate(pin_ID = paste(Position_ID, Pin_number, sep = "_")) %>% # Above all transposing and repositioning dataframe.
	ungroup() %>% # Below- adding columns, renaming variables, and reordering rows.
	dplyr::rename(Date = Start_Date, Location_ID = Location_ID.x) %>%  # rename SET reading date
	group_by(pin_ID) %>% # group by pinID to 
	mutate(EstDate = min(Date)) %>%  # create a column identifying the EstDate (date of the first SET-MH station reading)
	ungroup() %>% 
	mutate(DecYear = round((((as.numeric(difftime(.$Date, .$EstDate, units = "days"))))/365),3)) %>% 
	mutate(Raw = as.numeric(Raw)) 

attr(SET.data.long, 'Datainfo') <-"Full SET dataset including all measures in a LONG format" # give dataframe some metadata attributes

#---------------------------------------------------------------------------------------------------------#

# Clean outliers and 'issues' at time of reading ----
# Two strategies- 1) drop complete time series for a pin that has an 'issue' at any one point in the series, 
# vs 2) drop only that data point that has an issue.
# Or a merger of both stratagies.... 
# For holes: drop the whole timeseries
# For others: drop only that value

	# Create a list of pins that have a note regarding any issue at all ----
		troublePins <- SET.data.long %>% ungroup() %>% 
			select(Notes, pin_ID) %>% 
			filter(complete.cases(.)) %>% # remove all pins that don't have a note.
			`attr<-`("Datainfo", "List of pins that have reported issues (holes, etc)")
		
		pinlistClean <- unique(troublePins$pin_ID)
		
		bigIssues <- c("Hole", "hole", "mussel", "Holr", "Shell", "Mussel", "edge of hole", "hole next to mussel")
		bigIssuePins <- troublePins %>% filter(Notes %in% bigIssues)

#---------------------------------------------------------------------------------------------------------#
	
# SET data cleaned of any pins that have an 'issue' in the timeseries- Most restrictive dataset strategy 1 from above. ----
		# Use this as the default dataset for ALL analysis moving forward.

	
SET.data.long <- SET.data.long %>%
		mutate(bigIssuePin = pin_ID %in% bigIssuePins$pin_ID) %>% # Add in a column indicating if that pin is on the list of issues
		filter(bigIssuePin == FALSE) %>% 
		group_by(pin_ID) %>% # reinforce that the grouping is based on pins
		arrange(Date) %>% 
		mutate(Change = as.numeric(Raw) - as.numeric(Raw[1]), 
		       incrementalChange = c(NA, diff(Change))) 
	
attr(SET.data.long, 'Datainfo') <- "Any pin with a 'history of an issue' has been dropped" # edit metadata



## @knitr SAdata
###
# Surface Accretion  -----
#
#Clean up SA.data dataframe and reshape; same as above but working on surface accretion data
# similar steps as SET.data

SA.data.long <- SA.data %>% 
	gather('measure', 'Accretion', Measure_1:Measure_6) %>% 
	filter(!is.na(Accretion)) %>% 
	select(Layer_ID, Layer_Label, Location_ID.x, Estab_Date, Start_Date, Accretion, Core_Type, Site_ID, Plot_Name, Organization) %>% 
	mutate(DecYear = round((((as.numeric(difftime(Start_Date, Estab_Date, units = "days"))))/365),3)) %>% 
	dplyr::rename(Date = Start_Date)





### All clear to hear-- 03Apr2017- AFS -----


```


The next step is to clean up some of the variables in the tables. 

We now have two complete datasets that contain all our relevant SET-MH data. One set contains SET data with location, date and pin height data. One has Surface Accretion data with location, date and accretion data.  
We need to continue cleaning up the variable names and reshape the way the data is stored in the dataframes.


By *'melting'* the SET and SA dataframes we will transpose or pivot the measurement data to allow for calculations of incremental change in accretion and elevation, the change in date and the cumulative change that has been measured. This technique relies on a split-apply-combine strategy common in data sciences. The process first groups the data by some variables and value(s). Each grouping of data is then processed through a set of functions or formulas and the results are then grouped back into the dataset to rebuild the original dataset. The grouping on the SET data is a combination of Study site, location within that study site, the direction of the SET reading arm and finally by each pin.  
Be sure to read the comments embedded in the R code blocks as they provide some additional commentary on what's taking place in the code. 



The same basic steps are taken for the Surface Accretion data. 



We now have two additional datasets that contain the SA and SET data in a transposed or *'long'* format 















