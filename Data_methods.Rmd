---
title: Surface Elevation Table- Marker Horizon (SET-MH) Data Handling and Analaysis
  in R
author: "Adam Starke"
subtitle: The Nature Conservancy - Long Island Chapter
output: pdf_document
---

```{r DocumentSetup, echo=FALSE, results='asis', message=FALSE}
load(".Rdata")
library(ProjectTemplate)
load.project()
```

## Current Data and Analysis File Structure
This document is intended to document the process used for database storage and data analysis of our SET-MH data. Our current workflow uses Access2010 to store the data and RStudio (running the R language) for data handling. R is a powerful, though often times overwhelming tool. It is hoped that this document will aid anyone unfamiliar with R through our current data processing methods.

### Data Storage

The SET-MH method of marsh monitoring generates a large set of data that needs to be carefully maintained and catalogued. Fortunately, there has been a substaintial effort put forth by the National Park Service's National Capital Region Network Inventory and Monitoring Program to create a user-friendly Access database (DB). This DB allows for easy field data entry, data browsing and the creation of summary plots. There are also methods for exporting data to other software if desired. Managing SET-MH data in such as system is critical.  
Although the statistical analyses needed to calculate trends in marsh elevation are somewhat basic the restructuring of the data to allow for analysis can be somewhat complex. With this as a characteristic of the data we decided to use the statistical programming language R. R is one of the most widely used programming languages and contains an enormous amount of tools and scripts (known as packages in R) that make these restructuring of data possible. The ability to create 'scripts' allow for quick updates to analysis after data entry. A number of other features also exist that make R a good choice, particulary the graphics capabilities and the support for creation of 'data-driven documents' which allows for reproducible research. This document is actually written with R code embedded within it. This allows one to actually run the code that performs the analysis along side the documentation of the code. This allows the user to describe and show the exact steps taken for data organization, restructuring and analysis. Actual R code run in this document will be highlighted and have a slight grey background. This code can be cut and pasted or run as a whole document.  

As mentioned the data are currently stored in an Access database located at:

```{r echo=FALSE, size= 8, tidy=TRUE, prompt=FALSE, comment=NA}
print(SET.DB.path)
```
This file is referred to as the 'backend database' which holds all the raw SET-MH data.The 'front-end' of the database contains the user interface (UI) and is linked to the backend allowing multiple 'front ends' all pointing to one dataset. It's suggested that the front-end UI be used for quick checks on data and it is where **ALL** data entry takes place. 

### Accessing Our Data Through R
Using the power of R's scripting language capabilities the quickest, most direct, way to update SET-MH analyses is to simply navigate to the folder containing the RStudio project file (SET-MH_Analysis.Rproj) currently located at: `r getwd()`.   

Opening this file in RStudio will load all the code associated with this project. Run the script 'MarshProject.R' by opening it and clicking the *'Source'* button in the top-right of the window. This will pull all the code from the open file and run it. You will see activity in the "Console" window confirming that it is running.  

The main feature in this script is the *ProjectTemplate* package command: load.project() which automatically runs (sources) scripts contained in the '/data' folder, then cleans it up using scripts in the '/munge' folder. Munging simply means to remove irrelevant or missing values, restructure the data, transforming variable names etc. The scripts in each of these folders will be explained below. There are then three other lines of code that source code that performs the actual analysis and creates summary tables and plots. This will also be explained in detail below.  

## Data Processing Steps

This section will walk through each of our data processing steps; from loading data into R to exporting plots and summary tables. Most of these code blocks have been split to allow for commentary on what the code is doing. Again, any text that is bounded and highlighted is actual R code that is run in the software. If needed this text can be copied and pasted into an active R console and run.  

#### Data Loading  

The first step is to connect to the data. To do so R needs to communicate with the Access database through an *"ODBC- protocol"* or translator. For this we use the package **RODBC** which allows R to talk with Access. 

```{r ReadChunks, echo=FALSE}
# Read chunks pulls in the source files listed below and tags them based on the first line of the source file (ex. '@knitr Data'). 
knitr::read_chunk("data/SET_data_imports.R")
knitr::read_chunk("munge/Data_munge.R")
knitr::read_chunk("src/ElevationTrends_alternativeTechnique.R")
knitr::read_chunk("src/Summaries.R")
```

```{r DataLoad, results='hide'}
```
This code block creates a pathway to the Access database that is then used to communicate with the database. Once the connection is established several tables will be pulled into R as 'dataframes'; R's equivelent to a spreadsheet.   

```{r DataTableLoad}
```

This code block will load 7 tables into R containing:  
* Sites- data associated with each study site, i.e. general location, name, date of establishment  
* Locations- data associated with each station within each study site   
* Events- dates that samples were made  
* Positions- ID's for each SET arm position  
* SETdata- SET readings raw data  
* SAccret- Marker Horizon (MH) surface accretion raw data  
* SA_Layers- MH layer ID information  

#### Munging
Data munging (aka data wrangling) is the process of formatting and rearranging the 'shape' of the data to allow for analysis. As mentioned earlier the actual analysis of SET-MH data is not that complex. The primary crux of the analysis is getting the data into a form that makes the analysis programatically easy. The basic steps are to merge the tables brought in from the DB, retaining all relevant data from each table and then reshaping the data to allow for the analysis to run quickly. 

```{r MungeTables}
```

The next step is to clean up some of the variables in the tables. 

We now have two complete datasets that contain all our relevant SET-MH data. One set contains SET data with location, date and pin height data. One has Surface Accretion data with location, date and accretion data.  
We need to continue cleaning up the variable names and reshape the way the data is stored in the dataframes.

```{r MungeSETVariables}
```

By *'melting'* the SET and SA dataframes we will transpose or pivot the measurement data to allow for calculations of incremental change in accretion and elevation, the change in date and the cumulative change that has been measured. This technique relies on a split-apply-combine strategy common in data sciences. The process first groups the data by some variables and value(s). Each grouping of data is then processed through a set of functions or formulas and the results are then grouped back into the dataset to rebuild the original dataset. The grouping on the SET data is a combination of Study site, location within that study site, the direction of the SET reading arm and finally each pin.  
Be sure to read the comments embedded in the R code blocks as they provide some additional commentary on what's taking place in the code. 


```{r Dates}
```

The same basic steps are taken for the Surface Accretion data. 

```{r SAdata}

```

We now have two additional datasets that contain the SA and SET data in a transposed or *'long'* format 















